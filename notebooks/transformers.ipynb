{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ede5a2e",
   "metadata": {},
   "source": [
    "# Time Series Transformers\n",
    "### We aim to generate a transformer-based structure that generates an embedding from a 1024 time sequences obtained from our database generated with [this matlab code](https://github.com/soykilian/Signal-Generator.git)\n",
    "### Also following [this tutorial](https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4d0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 10:45:28.421324: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size : int):\n",
    "        super(Time2Vec, self).__init__(trainable=True,name='Time2VecLayer')\n",
    "        self.k = kernel_size \n",
    "    def build(self, input_shape : int = 1024):\n",
    "        self.wb = self.add_weight(name='wb', shape=(input_shape[1],), initializer='uniform', trainable=True)\n",
    "        self.bb = self.add_weight(name='bb', shape=(input_shape[1],), initializer='uniform', trainable=True)\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = np.dot(inputs, self.wa) + self.ba\n",
    "        wgts =np.sin(dp)\n",
    "        ret = np.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = np.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    def compute_output_shape(self, input_shape : int):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8934d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(keras.Model):\n",
    "    def __init__(self, name : str = 'AttentionBlock',\n",
    "                 num_heads : int =2,\n",
    "                 head_size : int = 128,\n",
    "                ff_dim : int = None,\n",
    "                dropout : int = 0,\n",
    "                **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=128, dropout=dropout)\n",
    "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
    "        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "        self.norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def build(self, input_shape : int):\n",
    "        self.conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.attention([inputs])\n",
    "        x = self.attention_dropout(x)\n",
    "        x = self.attention_norm(inputs + x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(inputs + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa1bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrunk(keras.Model):\n",
    "    def __init__(self, name='ModelTrunk',\n",
    "                 time2vec_dim=1,\n",
    "                 num_heads=2,\n",
    "                 head_size=128,\n",
    "                 ff_dim=None,\n",
    "                 num_layers=1,\n",
    "                 dropout=0,\n",
    "                **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range (num_layers)]\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        time_embeddings = keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
    "        x = np.concatenate([inputs,time_embeddings ])\n",
    "        for layer in self.attention_layers :\n",
    "            x = layer(x)\n",
    "        return np.reshape(x, (-1, x.shape[1] * x.shape[2]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "callbacks = [keras.callbacks.LearningRateScheduler(schedule=lr_scheduler, verbose=0)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ModelTrunk()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090938d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}