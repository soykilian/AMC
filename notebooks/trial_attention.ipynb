{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Add, Conv1D,Convolution2D, Bidirectional, LSTM, GRU, AlphaDropout, MaxPooling1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import sklearn, json\n",
    "import scipy.io as io\n",
    "from typing import Any, Dict\n",
    "sys.path.insert(0, \"/home/maria/AMC/includes\")\n",
    "import clr_callback\n",
    "\n",
    "path = '/home/maria/'\n",
    "dataset_path = path + 'dataset_1d/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with h5py.File(dataset_path +'X_train.mat', 'r') as f:\n",
    "    X_train = np.array(f['X_train']).T\n",
    "print(X_train.shape)\n",
    "with h5py.File(dataset_path +'X_test.mat', 'r') as f:\n",
    "    X_test = np.array(f['X_test']).T\n",
    "    lbl_train = io.loadmat(dataset_path +'lbl_train.mat')['lbl_train']\n",
    "lbl_test = io.loadmat(dataset_path +'lbl_test.mat')['lbl_test']\n",
    "print(lbl_test.shape)\n",
    "print(lbl_train.shape)\n",
    "Y_train = io.loadmat(dataset_path +'Y_train.mat')\n",
    "Y_train = Y_train['Y_train']\n",
    "print(Y_train.shape)\n",
    "Y_test = io.loadmat(dataset_path +'Y_test.mat')\n",
    "Y_test = Y_test['Y_test']\n",
    "print(Y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = ['LFM', '2FSK', '4FSK', '8FSK', 'Costas', '2PSK', '4PSK', '8PSK', 'Barker', 'Huffman', 'Frank', 'P1', 'P2',\n",
    "           'P3', 'P4', 'Px', 'Zadoff-Chu', 'T1', 'T2', 'T3', 'T4', 'NM', 'ruido']\n",
    "I_x = X_train[:, :, 0]\n",
    "Q_x = X_train[:, :, 1]\n",
    "X_train[:,:,1] = np.arctan(Q_x, I_x)/ np.pi\n",
    "X_train[:,:,0] = np.abs(I_x + 1j*Q_x)\n",
    "I_t = X_test[:, :, 0]\n",
    "Q_t = X_test[:, :, 1]\n",
    "X_test[:, :, 0] = np.arctan(Q_t, I_t)/np.pi\n",
    "X_test[:, :, 1] = np.abs(I_t + 1j*Q_t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "X_train, Y_train, lbl_train = sklearn.utils.shuffle(X_train[:],Y_train[:], lbl_train[:], random_state=2022)\n",
    "X_test, Y_test, lbl_test = sklearn.utils.shuffle(X_test[:], Y_test[:], lbl_test[:], random_state=2022)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionBlock(keras.Model):\n",
    "    def __init__(self, name : str = 'AttentionBlock',\n",
    "                 key_dim : int,\n",
    "                 num_heads : int =2,\n",
    "                 head_size : int = 128,\n",
    "                ff_dim : int = None,\n",
    "                dropout : int = 0,\n",
    "                **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=128, dropout=dropout)\n",
    "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
    "        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "        self.norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def build(self, input_shape : int):\n",
    "        self.conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        print(\"TRAZA\")\n",
    "        print(inputs)\n",
    "        x = self.attention(inputs)\n",
    "        print(\"attention\")\n",
    "        x = self.attention_dropout(x)\n",
    "        print(\"attention dropout\")\n",
    "        x = self.attention_norm(inputs + x)\n",
    "        print(\"attention norm\")\n",
    "        x = self.conv1(x)\n",
    "        print(\"conv1\")\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv2\")\n",
    "        x = self.dropout(x)\n",
    "        print(\"dropout\")\n",
    "        x = self.norm(inputs + x)\n",
    "        print(\"norm\")\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACTIVATION = 'selu'\n",
    "HIDDEN = 32\n",
    "INITIALIZER = 'lecun_normal'\n",
    "\n",
    "def ModelTrunk(input_shape : int):\n",
    "    X_input = tf.keras.Input(input_shape)\n",
    "    attention_block = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "    x = attention_block(X_input, X_input)\n",
    "    attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    print(x.shape)\n",
    "    x = attention_norm(x + X_input)\n",
    "    conv1 = keras.layers.Conv1D(filters=2, kernel_size=5, padding=\"same\", activation='relu')\n",
    "    x = conv1(x)\n",
    "    print(x.shape)\n",
    "    dropout = keras.layers.Dropout(0.1)\n",
    "    x = dropout(x)\n",
    "    norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    x = norm(x + X_input)\n",
    "    print(x.shape)\n",
    "    \n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(x)\n",
    "    print(\"------------------------------CONV--------------------------------------------\")\n",
    "    print(conv.shape)\n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = MaxPooling1D(2)(conv)\n",
    "    print(\"Pooling\")\n",
    "    print(conv.shape)\n",
    "    conv =Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = keras.layers.MaxPooling1D(2)(conv)\n",
    "    print(\"Pooling\")\n",
    "    print(conv.shape)\n",
    "    print(\"-----------------------------ATTENTION---------------------------------------------\")\n",
    "    # recurrent with attention, this generates sequences\n",
    "    \"\"\"\n",
    "    recurrent_forward = LSTM(HIDDEN, return_sequences=True, name= 'lstm0')(conv)\n",
    "    print(recurrent_forward)\n",
    "    recurrent_backward =  LSTM(HIDDEN, return_sequences=True, name= 'lstm0')(TimeStepReverse()(conv))\n",
    "    print(recurrent_backward)\n",
    "    recurrent = keras.layers.Concatenate()(\n",
    "        [recurrent_forward, recurrent_backward])\n",
    "    \"\"\"\n",
    "    # Try wth 3 attention blocks at least for improvement\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    print(\"Pooling\")\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='selu')(x)\n",
    "    x = AlphaDropout(0.6)(x)\n",
    "    x = Dense(256, activation='selu')(x)\n",
    "    x = AlphaDropout(0.6)(x)\n",
    "    x = Dense(23, activation='softmax')(x)\n",
    "   \n",
    "    \n",
    "    print(x.shape)\n",
    "    model = Model(inputs = X_input, outputs = x)\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da70d50a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ACTIVATION = 'selu'\n",
    "HIDDEN = 32\n",
    "INITIALIZER = 'lecun_normal'\n",
    "\n",
    "def ModelTrunk(input_shape : int):\n",
    "    X_input = tf.keras.Input(input_shape)\n",
    "    attention_block = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "    x = attention_block(X_input, X_input)\n",
    "    attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    print(x.shape)\n",
    "    x = attention_norm(x + X_input)\n",
    "    conv1 = keras.layers.Conv1D(filters=2, kernel_size=5, padding=\"same\", activation='relu')\n",
    "    x = conv1(x)\n",
    "    print(x.shape)\n",
    "    dropout = keras.layers.Dropout(0.1)\n",
    "    x = dropout(x)\n",
    "    norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    x = norm(x + X_input)\n",
    "    print(x.shape)\n",
    "    \n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(x)\n",
    "    print(\"------------------------------CONV--------------------------------------------\")\n",
    "    print(conv.shape)\n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = MaxPooling1D(2)(conv)\n",
    "    print(\"Pooling\")\n",
    "    print(conv.shape)\n",
    "    conv =Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = Conv1D(HIDDEN, 5, strides=1, activation=ACTIVATION, padding=\"same\", data_format='channels_last')(conv)\n",
    "    print(conv.shape)\n",
    "    conv = keras.layers.MaxPooling1D(2)(conv)\n",
    "    print(\"Pooling\")\n",
    "    print(conv.shape)\n",
    "    print(\"-----------------------------ATTENTION---------------------------------------------\")\n",
    "    # recurrent with attention, this generates sequences\n",
    "    \"\"\"\n",
    "    recurrent_forward = LSTM(HIDDEN, return_sequences=True, name= 'lstm0')(conv)\n",
    "    print(recurrent_forward)\n",
    "    recurrent_backward =  LSTM(HIDDEN, return_sequences=True, name= 'lstm0')(TimeStepReverse()(conv))\n",
    "    print(recurrent_backward)\n",
    "    recurrent = keras.layers.Concatenate()(\n",
    "        [recurrent_forward, recurrent_backward])\n",
    "    \"\"\"\n",
    "    # Try wth 3 attention blocks at least for improvement\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    print(\"Pooling\")\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='selu')(x)\n",
    "    x = AlphaDropout(0.6)(x)\n",
    "    x = Dense(256, activation='selu')(x)\n",
    "    x = AlphaDropout(0.6)(x)\n",
    "    x = Dense(23, activation='softmax')(x)\n",
    "   \n",
    "    \n",
    "    print(x.shape)\n",
    "    model = Model(inputs = X_input, outputs = x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fff5ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "callbacks = [keras.callbacks.LearningRateScheduler(schedule=lr_scheduler, verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cb68eb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1024, 2)\n",
      "(None, 1024, 2)\n",
      "(None, 1024, 2)\n",
      "------------------------------CONV--------------------------------------------\n",
      "(None, 1024, 32)\n",
      "(None, 1024, 32)\n",
      "Pooling\n",
      "(None, 512, 32)\n",
      "(None, 512, 32)\n",
      "(None, 512, 32)\n",
      "Pooling\n",
      "(None, 256, 32)\n",
      "-----------------------------ATTENTION---------------------------------------------\n",
      "Pooling\n",
      "(None, 512, 2)\n",
      "(None, 23)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 1024, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_9 (MultiHe (None, 1024, 2)      46          input_16[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 1024, 2)      0           multi_head_attention_9[0][0]     \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 1024, 2)      4           tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 1024, 2)      22          layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 1024, 2)      0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (None, 1024, 2)      0           dropout_28[0][0]                 \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 1024, 2)      4           tf.__operators__.add_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 512, 2)       0           layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 512)          524800      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropout)  (None, 512)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 256)          131328      alpha_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropout)  (None, 256)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 23)           5911        alpha_dropout_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 662,115\n",
      "Trainable params: 662,115\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ModelTrunk(X_train.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70edbdf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(1e-7), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eae5621",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CyclicLR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m output_path \u001B[38;5;241m=\u001B[39m path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults/model_stft\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m clr_triangular \u001B[38;5;241m=\u001B[39m \u001B[43mCyclicLR\u001B[49m(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtriangular\u001B[39m\u001B[38;5;124m'\u001B[39m, base_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m, max_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m, step_size\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m \u001B[38;5;241m*\u001B[39m (X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m256\u001B[39m))\n\u001B[1;32m      3\u001B[0m c\u001B[38;5;241m=\u001B[39m[clr_triangular,ModelCheckpoint(filepath\u001B[38;5;241m=\u001B[39m output_path \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'CyclicLR' is not defined"
     ]
    }
   ],
   "source": [
    "output_path = path + 'Results/model_stft'\n",
    "clr_triangular = CyclicLR(mode='triangular', base_lr=1e-7, max_lr=1e-4, step_size= 4 * (X_train.shape[0] // 256))\n",
    "c=[clr_triangular,ModelCheckpoint(filepath= output_path +'best_model.h5', monitor='val_loss', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367e683",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs = 50, batch_size = 256, callbacks = c, validation_data=(X_test, Y_test))\n",
    "with open(output_path +'history_rnn.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "model_json = model.to_json()\n",
    "with open(output_path +'model_rnn.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a226851",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.Input(shape=[1024,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59ad3037",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "target = Input(shape=[1024,2])\n",
    "output_tensor = layer(target,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077dc410",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}